{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from os.path import join, expanduser\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeedlingDataset(Dataset):\n",
    "    def __init__(self, labels, root_dir, subset=False, transform=None):\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels.iloc[idx, 0]\n",
    "        fullname = join(self.root_dir, img_name)\n",
    "        image = Image.open(fullname).convert('RGB')\n",
    "        labels = self.labels.iloc[idx, 2]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, int(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Alu_Gobi',\n",
       " 1: 'Alu_Matar',\n",
       " 2: 'American_Pancakes',\n",
       " 3: 'Apple_Pie',\n",
       " 4: 'Arros_negre',\n",
       " 5: 'Arroz_con_huevo',\n",
       " 6: 'Arroz_con_pollo',\n",
       " 7: 'Avocado_Toast',\n",
       " 8: 'BLT_Sandwich',\n",
       " 9: 'Bacon_Egg_and_Cheese_Sandwich',\n",
       " 10: 'Bagels',\n",
       " 11: 'Baguette',\n",
       " 12: 'Baked_Ziti',\n",
       " 13: 'Banana_Bread',\n",
       " 14: 'Banana_Split',\n",
       " 15: 'Barbecue_Ribs',\n",
       " 16: 'Barfi',\n",
       " 17: 'Beef_Rendang_(rendang)',\n",
       " 18: 'Beef_Vindaloo',\n",
       " 19: 'Beefaroni',\n",
       " 20: 'Beignet',\n",
       " 21: 'Beignets',\n",
       " 22: 'Blanquette_de_Veau',\n",
       " 23: 'Bocadillo_de_carne',\n",
       " 24: 'Bocadillo_de_jamon',\n",
       " 25: 'Bocadillo_de_pollo',\n",
       " 26: 'Bocadillo_de_queso',\n",
       " 27: 'Boeuf_Bourguignon',\n",
       " 28: 'Bouillabaisse',\n",
       " 29: 'Bread_Pudding',\n",
       " 30: 'Breakfast_Burrito',\n",
       " 31: 'Brioche',\n",
       " 32: 'Brodetto_Di_Pesce',\n",
       " 33: 'Brownies',\n",
       " 34: 'Buffalo_Wings',\n",
       " 35: 'Burrito',\n",
       " 36: 'Butter_Chicken',\n",
       " 37: 'Cabrales',\n",
       " 38: 'California-Style_Pizza',\n",
       " 39: 'Cannele',\n",
       " 40: 'Carrot_Halwa',\n",
       " 41: 'Cassoulet',\n",
       " 42: 'Chaat_Papri',\n",
       " 43: 'Cham-Cham',\n",
       " 44: 'Chana_Masala',\n",
       " 45: 'Chapati',\n",
       " 46: 'Chausson_aux_Pommes',\n",
       " 47: 'Cheese_Dog',\n",
       " 48: 'Cheeseburger',\n",
       " 49: 'Cheesesteak',\n",
       " 50: 'Chiacchiere',\n",
       " 51: 'Chicago-Style_Deep_Dish_Pizza',\n",
       " 52: 'Chicken_65',\n",
       " 53: 'Chicken_Biriyani',\n",
       " 54: 'Chicken_Fried_Steak',\n",
       " 55: 'Chicken_Nuggets',\n",
       " 56: 'Chicken_Parmigiana',\n",
       " 57: 'Chicken_Tajine',\n",
       " 58: 'Chicken_Tikka',\n",
       " 59: 'Chicken_and_Waffles',\n",
       " 60: 'Chicken_with_Chestnuts',\n",
       " 61: 'Chili_Chicken',\n",
       " 62: 'Chili_Dog',\n",
       " 63: 'Chili_con_Carne',\n",
       " 64: 'Chimichanga',\n",
       " 65: 'Chinese_Chicken_Salad',\n",
       " 66: 'Chistorra',\n",
       " 67: 'Chocolate_Chip_Cookie',\n",
       " 68: 'Chocolate_Italian_Sponge_Cake',\n",
       " 69: 'Chocolate_caliente',\n",
       " 70: 'Chop_Suey',\n",
       " 71: 'Chouquette',\n",
       " 72: 'Club_Sandwich',\n",
       " 73: 'Cobb_Salad',\n",
       " 74: 'Conejo_con_arroz',\n",
       " 75: 'Coq_au_Vin',\n",
       " 76: 'Cordero_asado',\n",
       " 77: 'Coriander_Chutney',\n",
       " 78: 'Corn_on_the_Cob',\n",
       " 79: 'Cornbread',\n",
       " 80: 'Couscous',\n",
       " 81: 'Crab_Cake',\n",
       " 82: 'Creamy_Chicken_Marsala',\n",
       " 83: 'Creamy_Lemon_Parmesan_Chicken_Piccata',\n",
       " 84: 'Creamy_Mushroom_Soup_With_Italian_Sausage',\n",
       " 85: 'Creme_Brulee',\n",
       " 86: 'Crepe',\n",
       " 87: 'Croissant',\n",
       " 88: 'Croque_Monsieur',\n",
       " 89: 'Croquetas_de_bacalao',\n",
       " 90: 'Croquetas_de_jamon',\n",
       " 91: 'Cuban_Sandwich',\n",
       " 92: 'Cupcake',\n",
       " 93: 'Daifuku',\n",
       " 94: 'Daiwa_Sushi',\n",
       " 95: 'Dal_Makhani',\n",
       " 96: 'Dango',\n",
       " 97: 'Date_Tamarind_Chutney',\n",
       " 98: 'Dessert_Bars',\n",
       " 99: 'Deviled_Eggs',\n",
       " 100: 'Dhokla',\n",
       " 101: 'Doughnut',\n",
       " 102: 'Easy_Italian_Stuffed_Peppers',\n",
       " 103: 'Ebi_furai',\n",
       " 104: 'Eclair',\n",
       " 105: 'Eggplant_And_Italian_Sausage_Gratin',\n",
       " 106: 'Eggs_Benedict',\n",
       " 107: 'Empanada_Galleg',\n",
       " 108: 'Endo_Sushi',\n",
       " 109: 'Escargot',\n",
       " 110: 'Espetos',\n",
       " 111: 'Fajitas',\n",
       " 112: 'Fish_Fry',\n",
       " 113: 'Flan',\n",
       " 114: 'Focaccia_Di_Genova',\n",
       " 115: 'French_Onion_Soup',\n",
       " 116: 'Fried Chicken',\n",
       " 117: 'Frozen_Yogurt',\n",
       " 118: 'Frutti_Di_Mare',\n",
       " 119: 'Fudge',\n",
       " 120: 'Fuet',\n",
       " 121: 'Gachas',\n",
       " 122: 'Galette_Complet',\n",
       " 123: 'Gambas_a_la_plancha',\n",
       " 124: 'Garlic_Butter_Italian_Sausage_Sandwiches',\n",
       " 125: 'Garlic_Parmesan_Cheese_Bombs',\n",
       " 126: 'Garlic_Soya_Chicken',\n",
       " 127: \"General_Tso's_Chicken\",\n",
       " 128: 'Gougere',\n",
       " 129: 'Gratin_Dauphinois',\n",
       " 130: 'Grilled_Cheese',\n",
       " 131: 'Gulab_Jamun',\n",
       " 132: 'Gumbo',\n",
       " 133: 'Gyoza',\n",
       " 134: 'Hakata_ramen',\n",
       " 135: 'Himono',\n",
       " 136: 'Hoagie',\n",
       " 137: 'Honey_Chilli_Potato',\n",
       " 138: 'Hot_Italian_Sliders',\n",
       " 139: 'Houtou_Fudou',\n",
       " 140: 'Ice_Cream_Float',\n",
       " 141: 'Idli',\n",
       " 142: 'Italian_Beef',\n",
       " 143: 'Italian_Braised_Chicken',\n",
       " 144: 'Italian_Chicken_Meal_Prep_Bowls',\n",
       " 145: 'Italian_Crescent_Casserole',\n",
       " 146: 'Italian_Garlic_Bread_Grilled_Cheese',\n",
       " 147: 'Italian_Green_Salad',\n",
       " 148: 'Italian_Oven_Roasted_Vegetables',\n",
       " 149: 'Italian_Ravioli',\n",
       " 150: 'Italian_Roasted_Potatoes',\n",
       " 151: 'Italian_Sandwich_Roll-Ups',\n",
       " 152: 'Italian_Sausage_And_Peppers',\n",
       " 153: 'Italian_Sausage_Rigatoni',\n",
       " 154: 'Italian_Seafood_Pasta_With_Mussels_&_Calamari',\n",
       " 155: 'Italian_Skillet_Chicken_With_Tomatoes_And_Mushrooms',\n",
       " 156: 'Italian_Style_Chicken_Mozzarella_Skillet',\n",
       " 157: 'Italian_Wedding_Soup',\n",
       " 158: 'Jabugo',\n",
       " 159: 'Jalebi',\n",
       " 160: 'Jambalaya',\n",
       " 161: 'Jambon-Beurre',\n",
       " 162: 'Kaiseki',\n",
       " 163: 'Kaisendon',\n",
       " 164: 'Kansas_City-Style_Barbecue',\n",
       " 165: 'Karaage',\n",
       " 166: 'Kasutera',\n",
       " 167: 'Katsukura',\n",
       " 168: 'Kayu',\n",
       " 169: 'Key_Lime_Pie',\n",
       " 170: 'Kheema',\n",
       " 171: 'Kheer',\n",
       " 172: 'Korokke',\n",
       " 173: 'Kulfi',\n",
       " 174: 'Kung_Pao_Chicken',\n",
       " 175: 'Kushiyaki',\n",
       " 176: 'Ladoo',\n",
       " 177: 'Lamb_Kebabs',\n",
       " 178: 'Lamb_Vindaloo',\n",
       " 179: 'Lime_Pickle',\n",
       " 180: 'Lle_Flottante',\n",
       " 181: 'Lobster_Roll',\n",
       " 182: 'Longaniza',\n",
       " 183: 'Mac and Cheese',\n",
       " 184: 'Macaron',\n",
       " 185: 'Madeleine',\n",
       " 186: 'Magdalenas',\n",
       " 187: 'Magret_de_canard',\n",
       " 188: 'Maisen',\n",
       " 189: 'Makizushi',\n",
       " 190: 'Malai_Kofta_(Veggie_Balls_With_Sauce)',\n",
       " 191: 'Mango_Lassi',\n",
       " 192: 'Marinara_Sauce',\n",
       " 193: 'Masala_Chai',\n",
       " 194: 'Masala_Dosa',\n",
       " 195: 'Masoor_Dal',\n",
       " 196: 'Meatloaf',\n",
       " 197: 'Medu_Vada',\n",
       " 198: 'Milkshake',\n",
       " 199: 'Mille_Feuille',\n",
       " 200: 'Miso_Soup',\n",
       " 201: 'Mofongo',\n",
       " 202: 'Molten_Chocolate_Cake',\n",
       " 203: 'Monaka',\n",
       " 204: 'Montadito',\n",
       " 205: 'Morcilla_de_Burgos',\n",
       " 206: 'Moules_Frites',\n",
       " 207: 'Mussels_In_Spicy_Red_Arrabbiata_Sauce',\n",
       " 208: 'Naan',\n",
       " 209: 'Napolitana_de_chocolate',\n",
       " 210: 'Nashville_Hot_Chicken',\n",
       " 211: 'Natillas',\n",
       " 212: 'Navratan_Korma_(Nine_Gem_Curry)',\n",
       " 213: 'Negima_yakitori',\n",
       " 214: 'New_England_Clam_Chowder',\n",
       " 215: 'New_York-Style_Cheesecake',\n",
       " 216: 'New_York-Style_Pizza',\n",
       " 217: 'Okonomiyaki',\n",
       " 218: 'Omelette',\n",
       " 219: 'Omurice',\n",
       " 220: 'Onion_Pakora',\n",
       " 221: 'Onion_Rings',\n",
       " 222: 'Orzo_With_Italian_Sausage_And_Peppers',\n",
       " 223: 'Osso_Buco',\n",
       " 224: 'Oyakodon',\n",
       " 225: 'Pa_amb_tomaquet',\n",
       " 226: 'Pain_au_Chocolat',\n",
       " 227: 'Palmie',\n",
       " 228: 'Pane_Bianco',\n",
       " 229: 'Papadum',\n",
       " 230: 'Pasta_Salad',\n",
       " 231: 'Pastrami_on_Rye',\n",
       " 232: 'Pecan_Pie',\n",
       " 233: 'Pernil',\n",
       " 234: 'Pesto_Sliders',\n",
       " 235: 'Pissaladiere',\n",
       " 236: 'Pisto',\n",
       " 237: \"Po'Boy\",\n",
       " 238: 'Poke_Salad',\n",
       " 239: 'Porchetta',\n",
       " 240: 'Pot_Pie',\n",
       " 241: 'Pot_Roast',\n",
       " 242: 'Pot_au_feu',\n",
       " 243: 'Pulao',\n",
       " 244: 'Pulled_Pork',\n",
       " 245: 'Queso_Tetilla',\n",
       " 246: 'Quiche',\n",
       " 247: 'Quince_Paste',\n",
       " 248: 'Quinoi',\n",
       " 249: 'Rabas',\n",
       " 250: 'Ragu_Alla_Bolognese',\n",
       " 251: 'Raita',\n",
       " 252: 'Rajma',\n",
       " 253: 'Ras_Malai',\n",
       " 254: 'Ratatouille',\n",
       " 255: 'Ravioli_With_Garlic_Basil_Oil',\n",
       " 256: 'Red_Velvet_Cake',\n",
       " 257: 'Reuben',\n",
       " 258: 'Rillettes',\n",
       " 259: 'Roast_Beef_Sandwich',\n",
       " 260: 'Rogan_Josh',\n",
       " 261: 'Romesco',\n",
       " 262: 'Roscon_de_Reyes',\n",
       " 263: \"S'more\",\n",
       " 264: 'Saag_Paneer',\n",
       " 265: 'Sambar',\n",
       " 266: 'Samosa',\n",
       " 267: 'Sauce_Bechamel',\n",
       " 268: 'Shoyu_ramen',\n",
       " 269: 'Shrimp_And_Corn_Risotto_With_Bacon',\n",
       " 270: 'Shrimp_and_Grits',\n",
       " 271: 'Sicilian_Pizza',\n",
       " 272: 'Slow_Cooker_Italian_Beef',\n",
       " 273: 'Snickerdoodle',\n",
       " 274: 'Soba',\n",
       " 275: 'Sofrito',\n",
       " 276: 'Souffle',\n",
       " 277: 'Spaghetti_Olio_E_Aglio',\n",
       " 278: 'Spaghetti_and_Meatballs',\n",
       " 279: 'Steak_Tartare',\n",
       " 280: 'Stromboli',\n",
       " 281: 'Stuffed_Italian_Flank_Steak',\n",
       " 282: 'Submarine_Sandwich',\n",
       " 283: 'Sukiyaki',\n",
       " 284: 'Sundae',\n",
       " 285: 'Surf_and_Turf',\n",
       " 286: 'Tandoori_Chicken',\n",
       " 287: 'Tapenade',\n",
       " 288: 'Tarta_de_Santiago',\n",
       " 289: 'Tarte',\n",
       " 290: 'Tartine',\n",
       " 291: 'Teppanyaki',\n",
       " 292: 'Teriyaki',\n",
       " 293: 'Texas-Style_Barbecue',\n",
       " 294: 'Tomate_Farcie',\n",
       " 295: 'Tomate_frito',\n",
       " 296: 'Tomato_&_Basil_Bruschetta',\n",
       " 297: 'Tonkotsu_ramen',\n",
       " 298: 'Torta_del_Casar',\n",
       " 299: 'Tortellini_Pasta_Carbonara',\n",
       " 300: 'Tortellini_Soup_With_Italian_Sausage_And_Spinach',\n",
       " 301: 'Tsunahachi',\n",
       " 302: 'Turron',\n",
       " 303: 'Udon',\n",
       " 304: 'Uthapam',\n",
       " 305: 'Vegetable_Jalfrezi',\n",
       " 306: 'Verdejo',\n",
       " 307: 'Yakisoba'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './archive/dataset/'\n",
    "cache_dir = expanduser(join('.', 'cache'))\n",
    "\n",
    "image_size = 224\n",
    "batch_size = 64\n",
    "# увеличил batch_size\n",
    "classes = listdir(data_dir + 'train/')\n",
    "classes = sorted(classes, key=lambda item: (int(item.partition(' ')[0])\n",
    "                               if item[0].isdigit() else float('inf'), item))\n",
    "num_to_class = dict(zip(range(len(classes)), classes))\n",
    "num_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>category</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alu_Gobi/Image_277.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alu_Gobi/Image_136.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alu_Gobi/Image_105.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alu_Gobi/Image_238.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alu_Gobi/Image_433.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121848</th>\n",
       "      <td>Yakisoba/Image_474.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121849</th>\n",
       "      <td>Yakisoba/Image_39.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121850</th>\n",
       "      <td>Yakisoba/Image_77.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121851</th>\n",
       "      <td>Yakisoba/Image_148.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121852</th>\n",
       "      <td>Yakisoba/Image_298.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121853 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          file  category  category_id\n",
       "0       Alu_Gobi/Image_277.jpg  Alu_Gobi            0\n",
       "1       Alu_Gobi/Image_136.jpg  Alu_Gobi            0\n",
       "2       Alu_Gobi/Image_105.jpg  Alu_Gobi            0\n",
       "3       Alu_Gobi/Image_238.jpg  Alu_Gobi            0\n",
       "4       Alu_Gobi/Image_433.jpg  Alu_Gobi            0\n",
       "...                        ...       ...          ...\n",
       "121848  Yakisoba/Image_474.jpg  Yakisoba          307\n",
       "121849   Yakisoba/Image_39.jpg  Yakisoba          307\n",
       "121850   Yakisoba/Image_77.jpg  Yakisoba          307\n",
       "121851  Yakisoba/Image_148.jpg  Yakisoba          307\n",
       "121852  Yakisoba/Image_298.jpg  Yakisoba          307\n",
       "\n",
       "[121853 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = []\n",
    "for index, label in enumerate(classes):\n",
    "    path = data_dir + 'train/' + label + '/'\n",
    "    for file in listdir(path):\n",
    "        train.append(['{}/{}'.format(label, file), label, index])\n",
    "    \n",
    "train_data = pd.DataFrame(train, columns=['file', 'category', 'category_id',]) \n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>category</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alu_Gobi/Image_178.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alu_Gobi/Image_457.JPG</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alu_Gobi/Image_212.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alu_Gobi/Image_40.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alu_Gobi/Image_90.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15228</th>\n",
       "      <td>Yakisoba/Image_5.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15229</th>\n",
       "      <td>Yakisoba/Image_365.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15230</th>\n",
       "      <td>Yakisoba/Image_31.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15231</th>\n",
       "      <td>Yakisoba/Image_147.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15232</th>\n",
       "      <td>Yakisoba/Image_377.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15233 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file  category  category_id\n",
       "0      Alu_Gobi/Image_178.jpg  Alu_Gobi            0\n",
       "1      Alu_Gobi/Image_457.JPG  Alu_Gobi            0\n",
       "2      Alu_Gobi/Image_212.jpg  Alu_Gobi            0\n",
       "3       Alu_Gobi/Image_40.jpg  Alu_Gobi            0\n",
       "4       Alu_Gobi/Image_90.jpg  Alu_Gobi            0\n",
       "...                       ...       ...          ...\n",
       "15228    Yakisoba/Image_5.jpg  Yakisoba          307\n",
       "15229  Yakisoba/Image_365.jpg  Yakisoba          307\n",
       "15230   Yakisoba/Image_31.jpg  Yakisoba          307\n",
       "15231  Yakisoba/Image_147.jpg  Yakisoba          307\n",
       "15232  Yakisoba/Image_377.jpg  Yakisoba          307\n",
       "\n",
       "[15233 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = []\n",
    "for index, label in enumerate(classes):\n",
    "    path = data_dir + 'val/' + label + '/'\n",
    "    for file in listdir(path):\n",
    "        val.append(['{}/{}'.format(label, file), label, index])\n",
    "    \n",
    "valid_data = pd.DataFrame(val, columns=['file', 'category', 'category_id',]) \n",
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>category</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alu_Gobi/Image_193.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alu_Gobi/Image_63.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alu_Gobi/Image_73.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alu_Gobi/Image_28.JPG</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alu_Gobi/Image_175.jpg</td>\n",
       "      <td>Alu_Gobi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15246</th>\n",
       "      <td>Yakisoba/Image_45.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15247</th>\n",
       "      <td>Yakisoba/Image_26.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15248</th>\n",
       "      <td>Yakisoba/Image_401.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15249</th>\n",
       "      <td>Yakisoba/Image_456.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15250</th>\n",
       "      <td>Yakisoba/Image_87.jpg</td>\n",
       "      <td>Yakisoba</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15251 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         file  category  category_id\n",
       "0      Alu_Gobi/Image_193.jpg  Alu_Gobi            0\n",
       "1       Alu_Gobi/Image_63.jpg  Alu_Gobi            0\n",
       "2       Alu_Gobi/Image_73.jpg  Alu_Gobi            0\n",
       "3       Alu_Gobi/Image_28.JPG  Alu_Gobi            0\n",
       "4      Alu_Gobi/Image_175.jpg  Alu_Gobi            0\n",
       "...                       ...       ...          ...\n",
       "15246   Yakisoba/Image_45.jpg  Yakisoba          307\n",
       "15247   Yakisoba/Image_26.jpg  Yakisoba          307\n",
       "15248  Yakisoba/Image_401.jpg  Yakisoba          307\n",
       "15249  Yakisoba/Image_456.jpg  Yakisoba          307\n",
       "15250   Yakisoba/Image_87.jpg  Yakisoba          307\n",
       "\n",
       "[15251 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = []\n",
    "for index, label in enumerate(classes):\n",
    "    path = data_dir + 'test/' + label + '/'\n",
    "    for file in listdir(path):\n",
    "        test.append(['{}/{}'.format(label, file), label, index])\n",
    "    \n",
    "test_data = pd.DataFrame(test, columns=['file', 'category', 'category_id',]) \n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gp/Documents/made_project/env/lib/python3.8/site-packages/torchvision/transforms/transforms.py:697: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\n",
      "/home/gp/Documents/made_project/env/lib/python3.8/site-packages/torchvision/transforms/transforms.py:210: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "train_trans = transforms.Compose([\n",
    "    transforms.RandomSizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "valid_trans = transforms.Compose([\n",
    "    transforms.Scale(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_set = SeedlingDataset(train_data, data_dir + 'train/', transform = train_trans)\n",
    "valid_set = SeedlingDataset(valid_data, data_dir + 'val/', transform = valid_trans)\n",
    "test_set = SeedlingDataset(test_data, data_dir + 'test/', transform = valid_trans)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(train_loader.dataset), \n",
    "    'valid': len(valid_loader.dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    writer = SummaryWriter(f\"runs/FOOD/tensorboard\")\n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                # scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode   \n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_batch = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in tqdm(dataloaders[phase]):\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "                labels = labels.view(-1)\n",
    "                \n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                # running_loss += loss.data[0]\n",
    "                running_loss += loss.data.item()\n",
    "                \n",
    "                num_correct = torch.sum(preds == labels.data).cpu().numpy().sum()\n",
    "                running_train_acc = float((preds == labels.data).sum()) / float(inputs.shape[0])\n",
    "                \n",
    "                running_corrects += num_correct\n",
    "                # torch.sum(preds == labels.data)\n",
    "                running_batch +=1\n",
    "                \n",
    "                step += 1\n",
    "                writer.add_scalar(\"Tranning Loss\", loss, global_step=step)\n",
    "                writer.add_scalar(\"Training Accuracy\", running_train_acc, global_step=step)    \n",
    "                writer.add_scalar(\"Training LR\", optimizer.param_groups[0][\"lr\"], global_step=step)\n",
    "                \n",
    "            epoch_loss = running_loss / running_batch\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == \"valid\":\n",
    "                    scheduler.step(epoch_loss)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "\n",
    "model = models.resnet50(pretrained=False)\n",
    "\n",
    "#I recommend training with these layers unfrozen for a couple of epochs after the initial frozen training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(classes))\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=308, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./cache/models/resnet50-new_weights_4.pth\"))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer4.2.conv1.weight False\n",
      "layer4.2.bn1.weight False\n",
      "layer4.2.bn1.bias False\n",
      "layer4.2.conv2.weight False\n",
      "layer4.2.bn2.weight False\n",
      "layer4.2.bn2.bias False\n",
      "layer4.2.conv3.weight False\n",
      "layer4.2.bn3.weight False\n",
      "layer4.2.bn3.bias False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"layer4.2.\" in name:\n",
    "        print(name, param.requires_grad)\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight False\n",
      "bn1.weight False\n",
      "bn1.bias False\n",
      "layer1.0.conv1.weight False\n",
      "layer1.0.bn1.weight False\n",
      "layer1.0.bn1.bias False\n",
      "layer1.0.conv2.weight False\n",
      "layer1.0.bn2.weight False\n",
      "layer1.0.bn2.bias False\n",
      "layer1.0.conv3.weight False\n",
      "layer1.0.bn3.weight False\n",
      "layer1.0.bn3.bias False\n",
      "layer1.0.downsample.0.weight False\n",
      "layer1.0.downsample.1.weight False\n",
      "layer1.0.downsample.1.bias False\n",
      "layer1.1.conv1.weight False\n",
      "layer1.1.bn1.weight False\n",
      "layer1.1.bn1.bias False\n",
      "layer1.1.conv2.weight False\n",
      "layer1.1.bn2.weight False\n",
      "layer1.1.bn2.bias False\n",
      "layer1.1.conv3.weight False\n",
      "layer1.1.bn3.weight False\n",
      "layer1.1.bn3.bias False\n",
      "layer1.2.conv1.weight False\n",
      "layer1.2.bn1.weight False\n",
      "layer1.2.bn1.bias False\n",
      "layer1.2.conv2.weight False\n",
      "layer1.2.bn2.weight False\n",
      "layer1.2.bn2.bias False\n",
      "layer1.2.conv3.weight False\n",
      "layer1.2.bn3.weight False\n",
      "layer1.2.bn3.bias False\n",
      "layer2.0.conv1.weight False\n",
      "layer2.0.bn1.weight False\n",
      "layer2.0.bn1.bias False\n",
      "layer2.0.conv2.weight False\n",
      "layer2.0.bn2.weight False\n",
      "layer2.0.bn2.bias False\n",
      "layer2.0.conv3.weight False\n",
      "layer2.0.bn3.weight False\n",
      "layer2.0.bn3.bias False\n",
      "layer2.0.downsample.0.weight False\n",
      "layer2.0.downsample.1.weight False\n",
      "layer2.0.downsample.1.bias False\n",
      "layer2.1.conv1.weight False\n",
      "layer2.1.bn1.weight False\n",
      "layer2.1.bn1.bias False\n",
      "layer2.1.conv2.weight False\n",
      "layer2.1.bn2.weight False\n",
      "layer2.1.bn2.bias False\n",
      "layer2.1.conv3.weight False\n",
      "layer2.1.bn3.weight False\n",
      "layer2.1.bn3.bias False\n",
      "layer2.2.conv1.weight False\n",
      "layer2.2.bn1.weight False\n",
      "layer2.2.bn1.bias False\n",
      "layer2.2.conv2.weight False\n",
      "layer2.2.bn2.weight False\n",
      "layer2.2.bn2.bias False\n",
      "layer2.2.conv3.weight False\n",
      "layer2.2.bn3.weight False\n",
      "layer2.2.bn3.bias False\n",
      "layer2.3.conv1.weight False\n",
      "layer2.3.bn1.weight False\n",
      "layer2.3.bn1.bias False\n",
      "layer2.3.conv2.weight False\n",
      "layer2.3.bn2.weight False\n",
      "layer2.3.bn2.bias False\n",
      "layer2.3.conv3.weight False\n",
      "layer2.3.bn3.weight False\n",
      "layer2.3.bn3.bias False\n",
      "layer3.0.conv1.weight False\n",
      "layer3.0.bn1.weight False\n",
      "layer3.0.bn1.bias False\n",
      "layer3.0.conv2.weight False\n",
      "layer3.0.bn2.weight False\n",
      "layer3.0.bn2.bias False\n",
      "layer3.0.conv3.weight False\n",
      "layer3.0.bn3.weight False\n",
      "layer3.0.bn3.bias False\n",
      "layer3.0.downsample.0.weight False\n",
      "layer3.0.downsample.1.weight False\n",
      "layer3.0.downsample.1.bias False\n",
      "layer3.1.conv1.weight False\n",
      "layer3.1.bn1.weight False\n",
      "layer3.1.bn1.bias False\n",
      "layer3.1.conv2.weight False\n",
      "layer3.1.bn2.weight False\n",
      "layer3.1.bn2.bias False\n",
      "layer3.1.conv3.weight False\n",
      "layer3.1.bn3.weight False\n",
      "layer3.1.bn3.bias False\n",
      "layer3.2.conv1.weight False\n",
      "layer3.2.bn1.weight False\n",
      "layer3.2.bn1.bias False\n",
      "layer3.2.conv2.weight False\n",
      "layer3.2.bn2.weight False\n",
      "layer3.2.bn2.bias False\n",
      "layer3.2.conv3.weight False\n",
      "layer3.2.bn3.weight False\n",
      "layer3.2.bn3.bias False\n",
      "layer3.3.conv1.weight False\n",
      "layer3.3.bn1.weight False\n",
      "layer3.3.bn1.bias False\n",
      "layer3.3.conv2.weight False\n",
      "layer3.3.bn2.weight False\n",
      "layer3.3.bn2.bias False\n",
      "layer3.3.conv3.weight False\n",
      "layer3.3.bn3.weight False\n",
      "layer3.3.bn3.bias False\n",
      "layer3.4.conv1.weight False\n",
      "layer3.4.bn1.weight False\n",
      "layer3.4.bn1.bias False\n",
      "layer3.4.conv2.weight False\n",
      "layer3.4.bn2.weight False\n",
      "layer3.4.bn2.bias False\n",
      "layer3.4.conv3.weight False\n",
      "layer3.4.bn3.weight False\n",
      "layer3.4.bn3.bias False\n",
      "layer3.5.conv1.weight False\n",
      "layer3.5.bn1.weight False\n",
      "layer3.5.bn1.bias False\n",
      "layer3.5.conv2.weight False\n",
      "layer3.5.bn2.weight False\n",
      "layer3.5.bn2.bias False\n",
      "layer3.5.conv3.weight False\n",
      "layer3.5.bn3.weight False\n",
      "layer3.5.bn3.bias False\n",
      "layer4.0.conv1.weight False\n",
      "layer4.0.bn1.weight False\n",
      "layer4.0.bn1.bias False\n",
      "layer4.0.conv2.weight False\n",
      "layer4.0.bn2.weight False\n",
      "layer4.0.bn2.bias False\n",
      "layer4.0.conv3.weight False\n",
      "layer4.0.bn3.weight False\n",
      "layer4.0.bn3.bias False\n",
      "layer4.0.downsample.0.weight False\n",
      "layer4.0.downsample.1.weight False\n",
      "layer4.0.downsample.1.bias False\n",
      "layer4.1.conv1.weight False\n",
      "layer4.1.bn1.weight False\n",
      "layer4.1.bn1.bias False\n",
      "layer4.1.conv2.weight False\n",
      "layer4.1.bn2.weight False\n",
      "layer4.1.bn2.bias False\n",
      "layer4.1.conv3.weight False\n",
      "layer4.1.bn3.weight False\n",
      "layer4.1.bn3.bias False\n",
      "layer4.2.conv1.weight True\n",
      "layer4.2.bn1.weight True\n",
      "layer4.2.bn1.bias True\n",
      "layer4.2.conv2.weight True\n",
      "layer4.2.bn2.weight True\n",
      "layer4.2.bn2.bias True\n",
      "layer4.2.conv3.weight True\n",
      "layer4.2.bn3.weight True\n",
      "layer4.2.bn3.bias True\n",
      "fc.weight True\n",
      "fc.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=1)\n",
    "\n",
    "loaders = {'train':train_loader, 'valid':valid_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 534/1904 [03:20<09:30,  2.40it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 44%|████▎     | 831/1904 [05:24<07:00,  2.55it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 1904/1904 [12:31<00:00,  2.53it/s]\n",
      "  0%|          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3438 Acc: 0.9096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:40<00:00,  2.38it/s]\n",
      "  0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.3307 Acc: 0.9334\n",
      "Epoch 1/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1904 [00:06<13:28,  2.34it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 535/1904 [03:24<08:28,  2.69it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 34%|███▎      | 641/1904 [04:04<07:49,  2.69it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 1904/1904 [11:51<00:00,  2.67it/s]\n",
      "  0%|          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3470 Acc: 0.9101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:30<00:00,  2.65it/s]\n",
      "  0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.3237 Acc: 0.9348\n",
      "Epoch 2/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 315/1904 [01:59<10:01,  2.64it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 20%|██        | 389/1904 [02:27<09:24,  2.68it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 1458/1904 [08:55<02:25,  3.06it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 1904/1904 [11:24<00:00,  2.78it/s]\n",
      "  0%|          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3384 Acc: 0.9111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:27<00:00,  2.73it/s]\n",
      "  0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.3250 Acc: 0.9361\n",
      "Epoch 3/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 488/1904 [03:11<09:19,  2.53it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 32%|███▏      | 602/1904 [03:51<08:41,  2.49it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 1904/1904 [11:59<00:00,  2.65it/s]\n",
      "  0%|          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3376 Acc: 0.9127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:27<00:00,  2.74it/s]\n",
      "  0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.3239 Acc: 0.9360\n",
      "Epoch 4/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 267/1904 [01:40<10:54,  2.50it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 38%|███▊      | 730/1904 [04:37<07:38,  2.56it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 95%|█████████▍| 1800/1904 [11:20<00:42,  2.44it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 1904/1904 [12:00<00:00,  2.64it/s]\n",
      "  0%|          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3195 Acc: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:29<00:00,  2.66it/s]\n",
      "  0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.3129 Acc: 0.9367\n",
      "Epoch 5/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 156/1904 [01:00<11:19,  2.57it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 858/1904 [05:32<06:40,  2.61it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 1904/1904 [11:31<00:00,  2.75it/s]\n",
      "  0%|          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3125 Acc: 0.9184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:12<00:00,  3.28it/s]\n",
      "  0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.3100 Acc: 0.9379\n",
      "Epoch 6/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 401/1904 [02:10<08:09,  3.07it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 73%|███████▎  | 1396/1904 [07:32<02:42,  3.12it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 1904/1904 [10:15<00:00,  3.09it/s]\n",
      "  0%|          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3093 Acc: 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:10<00:00,  3.38it/s]\n",
      "  0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.3111 Acc: 0.9388\n",
      "Epoch 7/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 131/1904 [00:43<09:28,  3.12it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 29%|██▉       | 559/1904 [03:06<07:26,  3.01it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 36%|███▌      | 682/1904 [03:47<06:45,  3.01it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 1904/1904 [10:30<00:00,  3.02it/s]\n",
      "  0%|          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3072 Acc: 0.9199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:15<00:00,  3.15it/s]\n",
      "  0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.3103 Acc: 0.9376\n",
      "Epoch 8/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 416/1904 [02:17<08:33,  2.90it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 1904/1904 [10:59<00:00,  2.89it/s]\n",
      "  0%|          | 0/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.3073 Acc: 0.9199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:15<00:00,  3.18it/s]\n",
      "  0%|          | 0/1904 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid Loss: 0.3080 Acc: 0.9384\n",
      "Epoch 9/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 90/1904 [00:31<10:31,  2.87it/s]/home/gp/Documents/made_project/env/lib/python3.8/site-packages/PIL/Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "  7%|▋         | 137/1904 [00:48<10:21,  2.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-d582186f6f71>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloaders\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscheduler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m20\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-8-1451f37a00e5>\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(dataloaders, model, criterion, optimizer, scheduler, num_epochs)\u001B[0m\n\u001B[1;32m     53\u001B[0m                 \u001B[0;31m# statistics\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m                 \u001B[0;31m# running_loss += loss.data[0]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 55\u001B[0;31m                 \u001B[0mrunning_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m                 \u001B[0mnum_correct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpreds\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(loaders, model, criterion, optimizer, scheduler, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 239/239 [01:14<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP_1 acc:  0.9384958363386008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "\n",
    "for data in tqdm(loaders[\"test\"]):\n",
    "    model.train(False)\n",
    "    inputs, labels = data\n",
    "    labels = labels.view(-1)\n",
    "    \n",
    "    if use_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "    inputs = Variable(inputs)\n",
    "    labels = Variable(labels)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    test_acc.append((labels == preds).cpu().numpy())\n",
    "    \n",
    "print(\"TOP_1 acc: \", np.hstack(test_acc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"./cache/models/resnet50-new_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}